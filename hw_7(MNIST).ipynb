{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models \n",
    "from keras import layers\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data normalization\n",
    "X_train = X_train.reshape((60000, 28*28))\n",
    "X_train = X_train/255\n",
    "\n",
    "X_test = X_test.reshape((10000, 28*28))\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train, y_test data preprocessing\n",
    "y_train=to_categorical(y_train)\n",
    "y_test=to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Softmax regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_soft = time.time()\n",
    "net_soft = models.Sequential()\n",
    "net_soft.add(layers.Dense(10, activation='softmax', input_shape=(28*28,)))\n",
    "net_soft.compile(optimizer='adam',loss = 'categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 3s 53us/step - loss: 0.4694 - accuracy: 0.8772\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.3043 - accuracy: 0.9151\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.2836 - accuracy: 0.9209\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.2730 - accuracy: 0.9237\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.2668 - accuracy: 0.9260\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.2621 - accuracy: 0.9274\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.2583 - accuracy: 0.9284\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.2555 - accuracy: 0.9293\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.2529 - accuracy: 0.9298\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.2512 - accuracy: 0.9304\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.2493 - accuracy: 0.9315\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.2479 - accuracy: 0.9314\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.2465 - accuracy: 0.9317\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.2448 - accuracy: 0.9319\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.2443 - accuracy: 0.9327\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.2426 - accuracy: 0.9325\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - ETA: 0s - loss: 0.2425 - accuracy: 0.93 - 2s 40us/step - loss: 0.2420 - accuracy: 0.9335\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.2412 - accuracy: 0.9334\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.2404 - accuracy: 0.9338\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.2392 - accuracy: 0.9334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x22db2138b08>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_soft.fit(X_train, y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 34us/step\n"
     ]
    }
   ],
   "source": [
    "soft_loss, soft_acc = net_soft.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9273999929428101\n",
      "softmax time: 48.67577815055847\n"
     ]
    }
   ],
   "source": [
    "# softmax 방식으로 얻은 accuracy\n",
    "print(soft_acc)\n",
    "print('softmax time:', time.time()-start_soft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "(X_svm_train, y_svm_train), (X_svm_test, y_svm_test) = mnist.load_data()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# Data normalization\n",
    "X_svm_train = X_svm_train.reshape((60000, 28*28))\n",
    "X_svm_train = X_svm_train/255\n",
    "\n",
    "X_svm_test = X_svm_test.reshape((10000, 28*28))\n",
    "X_svm_test = X_svm_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data scaling \n",
    "X_svm_train_scaled = scaler.fit_transform(X_train.astype(np.float32))\n",
    "X_svm_test_scaled = scaler.fit_transform(X_test.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modeling\n",
    "start_svm = time.time()\n",
    "svc=SVC(kernel='linear')\n",
    "svc.fit(X_svm_train,y_svm_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9404\n",
      "svm time: 591.6522121429443\n"
     ]
    }
   ],
   "source": [
    "# svm 방식으로 얻은 accuracy\n",
    "y_svm_pred = svc.predict(X_svm_test)\n",
    "print(accuracy_score(y_svm_pred, y_svm_test))\n",
    "print('svm time:', time.time()-start_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modeling\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf  =  RandomForestClassifier(n_estimators = 200, n_jobs = -1)\n",
    "start_rf = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=200, n_jobs=-1)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9047\n",
      "rf time: 83.41883873939514\n"
     ]
    }
   ],
   "source": [
    "# Randomforest 방식으로 얻은 accuracy\n",
    "y_clf_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_clf_pred, y_test))\n",
    "print('rf time:', time.time()-start_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 결과 분석 및 고찰 \n" ,
    "softmax regression으로 얻은 정확도는 0.9265\n",
    "svm으로 얻은 정확도는 0.9404\n",
    "RandomForest로 얻은 정확도는 0.9043 이다. \n",
    "각각의 방법이 완전히 동일한 조건으로 둔 것이 아니라 이들을 단순히 정확도로 무엇이 더 우세하다고 말하긴 힘들다.\n",
    "하지만 각각의 방법들의 계산 속도를 보면 softmax 방식은 48초, svm은 대략 10분정도 소요했고, radomforest는 1분 40초가 소요되었다.\n",
    "아무리 높은 정확도를 가진 방법이라 해도 한번 시행에 너무 많은 시간이 걸리는 svm은 이 경우에는 비합리적이라 판단되어진다.\n",
    "따라서 이 경우 softmax 방식을 사용하는 것이 다른 두가지 방법보다는 좀더 합리적인 판단이라고 생각 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
